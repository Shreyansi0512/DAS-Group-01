---
title: "Group Project 2"
author: "Group 1 QIJIA HE, Jianan Liu, Subasish Behera,Shreyansi Jain, Fanting Kai"
date: "`r Sys.Date()`"
output:
  pdf_document:
    number_sections: yes
    toc: yes
  html_document:
    toc: yes
    df_print: paged
fig_caption: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, eval = TRUE, warning = FALSE, message = FALSE, comment = NA)
```

```{r libraries}
library(pROC)
library(relaimpo)
library(tidyverse) 
library(car)
library(kableExtra)
library(gridExtra)
library(skimr)
library(knitr)
library(broom)
```

``````{r data, echo = FALSE, eval = TRUE}
setwd("~/Desktop/Project 2/data_analysis")
dataset <- read_csv("dataset.csv")
```

# Introduction {#sec:intro}
The total number of people living in a household can be influenced by multiple variables. In certain cases, a specific type of variable may have a significant impact, which can be determined by different actual situations. By studying the correlation between variables, we can examine the living characteristics of local people.

In this dataset, we notice that six numeric variables are displayed, so we initially consider all of them. We plan to apply a generalized linear model to fit the data and discover the relevance of the variables.

At the same time, we should take into account the interaction between variables. The interaction between variables can sometimes provide insights that may not be evident when considering each variable independently. By including interaction terms in our generalized linear model, we can better understand the combined effects of these variables on the total number of people living in a household. This can help us develop a more accurate and comprehensive understanding of the factors that influence household size in the area under study.

This project begins with an expalnatory analysis into the data including data graphics and summaries in \ref{sec:eda}. The formal analysis has been procured in a GLM model at which will be seen in sections \ref{sec:glm} and \ref{sec:conc}. 

# Exploratory Data Analysis {#sec:EDA}
## Data Mining {#sec:sub}

```{r}
setwd("~/Desktop/Project 2/data_analysis")
dataset <- read_csv("dataset.csv")

library(kableExtra)
library(gridExtra)

str(dataset)
summary(dataset)

dataset$Electricity <- factor(dataset$Electricity)
dataset$Type.of.Household <- factor(dataset$Type.of.Household)
dataset$Household.Head.Sex <- factor(dataset$Household.Head.Sex)
df <- data.frame(dataset)
df_num <- select_if(df, is.numeric)
summary(df_num)

skim <- skim_with(numeric = sfl(hist = NULL), 
                  base = sfl(n = length))
skim(df_num) %>%
  transmute(Variable=skim_variable, n = n, Mean=numeric.mean, SD=numeric.sd,
            Min=numeric.p0, Median=numeric.p50,  Max=numeric.p100,
            IQR = numeric.p75-numeric.p50) %>%
  kable(caption = '\\label{tab: Summary Statistics} Asian Health Data Summary Statistics for 2015.', digits=2) %>%
  kable_styling(font_size = 10, latex_options = "hold_position")

```

To better understand the data at hand, various figures were created to provide visuals and illustrations. We begin with the scatterplots of each variable to understand the distribution of the data:
```{r, eval = TRUE, out.width = '80%', fig.align = "center", fig.cap = "\\label{fig:scatters} Scatterplots of Phillipine Family", fig.pos = "H"}
setwd("~/Desktop/Project 2/data_analysis")
DataSet <- read_csv("dataset.csv")
library(kableExtra)
library(gridExtra)
library(ggplot2)
library(GGally)

ggplot(DataSet, aes(x = Total.Household.Income, y = Total.Number.of.Family.members)) +
  geom_point() +
  geom_smooth(method = "lm") +
  ggtitle("Total Household Income vs. Number of Family Members")


ggplot(DataSet, aes(x = Total.Food.Expenditure, y = Total.Number.of.Family.members)) +
  geom_point() +
  geom_smooth(method = "lm") +
  ggtitle("Total Food Expenditure vs. Number of Family Members")

ggplot(DataSet, aes(x = Household.Head.Age, y = Total.Number.of.Family.members)) +
  geom_point() +
  geom_smooth(method = "lm") +
  ggtitle("Household Head Age vs. Number of Family Members")

ggplot(DataSet, aes(x = House.Floor.Area, y = Total.Number.of.Family.members)) +
  geom_point() +
  geom_smooth(method = "lm") +
  ggtitle("House Floor Area vs. Number of Family Members")

ggplot(DataSet, aes(x = House.Age, y = Total.Number.of.Family.members)) +
  geom_point() +
  geom_smooth(method = "lm") +
  ggtitle("House Age vs. Number of Family Members")

ggplot(DataSet, aes(x = Number.of.bedrooms, y = Total.Number.of.Family.members)) +
  geom_point() +
  geom_smooth(method = "lm") +
  ggtitle("Number of Bedrooms vs. Number of Family Members")

ggplot(data = DataSet, aes(x = Type.of.Household, y = Total.Number.of.Family.members)) +
  geom_boxplot() +
  ggtitle("Type of Household vs. Number of Family Members")

ggplot(data = DataSet, aes(x = Total.Food.Expenditure, y = Total.Number.of.Family.members, group = Electricity)) +
  geom_point() +
  geom_smooth(method = "lm") +
  ggtitle("Total Food Expenditure vs. Number of Family Members by Electricity") +
  labs(x = "Total Food Expenditure (in Philippine peso)", y = "Total Number of Family Members") +
  theme(plot.title = element_text(hjust = 0.5))
```

We proceed with the correlation matrix the highlight relationships, below:
```{r, eval = TRUE, out.width = '80%', fig.align = "center", fig.cap = "\\label{fig:scatters} Correlation Matrix of Phillipine Households", fig.pos = "H"}

ggpairs(DataSet[,-c(2, 4, 6, 11)])
```
It is evident from the corrmatrix plot correlation matrix that the greatest positive correlation exists between family members and food expenditure. 

```{r, eval = TRUE, out.width = '80%', fig.align = "center", fig.cap = "\\label{fig:histograms}  Plot histograms of the variables", fig.pos = "H"}

DataSet %>% select(Total.Household.Income, Total.Food.Expenditure,
Household.Head.Age, Total.Number.of.Family.members, House.Floor.Area,
House.Age, Number.of.bedrooms) %>% gather() %>% ggplot(aes(x=value)) +
geom_histogram(bins = 50) + facet_wrap(~key, scales="free")
```

Using the trends seen in the data exploration, our group has taken this further to examine causal relationships through generalized linear models in the Logistic regression.

# Logistic regression {#sec:DLR}
## Data logistic regression {sec;sub}
After drawing the scatter plot and data correlation, we trying to do a logistic regression from data. we dividing the dataset into training and testing sets, logistic regression models are fitted for each predictor variable and a full model using the glm function in R. The response variable in all the models is Total.Number.of.Family.members, and the predictor variables are Total.Household.Income, Total.Food.Expenditure, Household.Head.Age, House.Floor.Area, House.Age, and Number.of.bedrooms.
Multiple logistic regression models are also fitted for each predictor variable separately.

```{r include=FALSE}
   library(pROC)
dt = read.csv("~/Desktop/Project 2/dataset1.csv")
train_sub = sample(nrow(dt),7/10*nrow(dt))
train_data = dt[train_sub,]
test_data = dt[-train_sub,]

dt_logistic <- glm(Total.Number.of.Family.members ~ Total.Household.Income+Total.Food.Expenditure+Household.Head.Age+House.Floor.Area+House.Age+Number.of.bedrooms, data = train_data)
income_logistic <- glm(Total.Number.of.Family.members ~ Total.Household.Income, 
                   data = train_data)
food_logistic <- glm(Total.Number.of.Family.members ~ Total.Food.Expenditure, 
                       data = train_data)
age_logistic <- glm(Total.Number.of.Family.members ~ Household.Head.Age, 
                     data = train_data)
area_logistic <- glm(Total.Number.of.Family.members ~ House.Floor.Area, 
                     data = train_data)
bed_logistic <- glm(Total.Number.of.Family.members ~ Number.of.bedrooms, 
                     data = train_data)
house_age_logistic <- glm(Total.Number.of.Family.members ~ House.Age, 
                    data = train_data)
```
The predict function is then used to generate predicted probabilities for the test data using the full model and the type = "response" argument. And the table function is used to compare the predicted probabilities with the actual values in the test data. 
And a binary response variable is created by collapsing some levels of the original response variable, and the pROC package is used to calculate the ROC curve using the binary response variable and predicted probabilities. 
Finally, a Poisson GLM model is fit using all the predictor variables in the dataset with the glm function and family = poisson() argument, which specifies that the response variable has a Poisson distribution. The resulting model is stored in the Our_glm_model object for further examination
```{r}
pre_logistic<-predict(dt_logistic,newdata=test_data,type="response")
obs_p_logistic = data.frame(prob=pre_logistic,obs=test_data$Total.Number.of.Family.members)
test_data$binary_response <- ifelse(test_data$Total.Number.of.Family.members > 2, "Positive", "Negative")
pre_logistic <- predict(dt_logistic, newdata = test_data, type = "response")
logistic_roc <- roc(test_data$binary_response, pre_logistic)
plot(logistic_roc)

Our_glm_model <- glm(Total.Number.of.Family.members ~ Total.Household.Income + Total.Food.Expenditure + Household.Head.Age + House.Floor.Area + House.Age + Number.of.bedrooms, data = dataset, family = poisson())

Our_glm_model
```

## Show Logistic {#sec:sub}

> We get results listed below:
> 
```{R echo=FALSE}
summary(dt_logistic)
```

The deviance residuals measure how much probabilities estimated from our model differ from the observed proportions of successes. Bigger values indicate a bigger difference. Smaller values mean a smaller difference.
The summary of the logistic regression model shows the coefficient estimates, standard errors, z-values, and p-values for each predictor variable in the model. The deviance residuals measure the difference between the observed and predicted values, with larger values indicating a larger difference. Although the deviance residuals suggest a generally good fit of the model, the presence of a large maximum value suggests that there may be non-relevant variables included in the model. Hence, it is advisable to rank the importance of variables and remove any irrelevant variables from the model.


# Importance ranking of variables {sec:IRV}
```{R include=FALSE}
library(relaimpo);
```
```{R}
model <- glm(Total.Number.of.Family.members ~ Total.Household.Income + Total.Food.Expenditure + Household.Head.Age + House.Floor.Area + House.Age + Number.of.bedrooms, data = dataset)
summary(model)
rel_imp <- calc.relimp(model, type = "pratt")
rel_imp
importance <- rel_imp$pratt
importance
calc.relimp( Total.Number.of.Family.members~Total.Household.Income+Total.Food.Expenditure+Household.Head.Age+House.Floor.Area+House.Age+Number.of.bedrooms, data = train_data )
barplot(importance, main = "Relative Importance of Predictor Variables",
        xlab = "Predictor Variables", ylab = "Relative Importance",
        col = "steelblue", ylim = c(0,1))
```
function calc.relimp() calculates several relative importance metrics for the linear model. From the results displayed above, we witness two sets of important index.
Consistent to the logistic above, Total.Household.Income and Total.Food.Expenditure show greater vitality than other four indexes.House age and number of bedroom have a slightly impact. Now we first drop2 variables and then drop 2 slightly influenced variables and re-run de logistic and importance ranking to see the best we can do.

## Re-run the analysis for 4 variables {sec:sub}
```{R include=FALSE}
dt_logistic <- glm(Total.Number.of.Family.members ~ Total.Household.Income+Total.Food.Expenditure+House.Age+Number.of.bedrooms, data = train_data)
summary(dt_logistic)
```
```{R echo=FALSE}
calc.relimp( Total.Number.of.Family.members~Total.Household.Income+Total.Food.Expenditure+House.Age+Number.of.bedrooms, data = train_data )
```
Based on the output of the calc.relimp function, we can see that the total response variance for this variable is 5.363603, and the proportion of variance explained by this model is 22.68%. 

## Re-run the analysis for only 2 variables {sec:sub}
```{R include=FALSE}
dt_logistic <- glm(Total.Number.of.Family.members ~ Total.Household.Income+Total.Food.Expenditure, data = train_data)
summary(dt_logistic)
```
```{R echo=FALSE}
calc.relimp( Total.Number.of.Family.members~Total.Household.Income+Total.Food.Expenditure, data = train_data )
```
Based on the output of the calc.relimp function, we can see that the total response variance for this variable is 5.363603, and the proportion of variance explained by this model is 22.02%. Total.Food.Expenditure appears to be more important in explaining the variance in the Total.Number.of.Family.members, as its relative importance (lmg) is higher than that of Total.Household.Income.And the coefficients indicate the relationship between each predictor and the response variable (Total.Number.of.Family.members). In the 1X model, a unit increase in Total.Household.Income is associated with a 2.276022e-06 increase in Total.Number.of.Family.members, whereas a unit increase in Total.Food.Expenditure is associated with a 2.539690e-05 increase in Total.Number.of.Family.members. In the 2Xs model, the relationships are -1.510802e-06 and 3.125984e-05, respectively. 

# Conclusion {sec:Conc}
From the results we get above, we have concluded that Total.Household.Income and Total.Food.Expenditure are the two most important variables. However, the average coefficients are still relatively small. This can be attributed to several reasons. The most significant reason is that Income and Expenditure are not on the same scale as the number of family members. Consequently, a sharp increase or decrease in Income and Expenditure may not necessarily cause any change in the number of family members.

## Appendix {sec:sub}
Here, we discover the correlations between variables, in other words the columns of the dataset.

```{R include=FALSE}
shapiro.test(dt$Total.Household.Income)$p
shapiro.test(dt$Total.Food.Expenditure)$p
cor.test(dt$Total.Household.Income, dt$Total.Food.Expenditure, method = "pearson")
```

The results from above conducted on the dataset show that both p-values are extremely small, leading to the rejection of the null hypothesis (data is normally distributed). Consequently, neither Total.Household.Income nor Total.Food.Expenditure is normally distributed.The extremely small p-value in Pearson's product-moment correlation test reveals a statistically significant correlation between Total.Household.Income and Total.Food.Expenditure. The 95% confidence interval for this correlation is [0.5810680, 0.6402141], suggesting that the true correlation between these two variables lies within this range. And the sample estimate for the correlation (cor) is 0.6114945, indicating a positive and moderately strong relationship between Total.Household.Income and Total.Food.Expenditure.

```{R include=FALSE}
if (!requireNamespace("ggstatsplot", quietly = TRUE)) {
  install.packages("ggstatsplot")
}
library(ggstatsplot)

```
```{R echo=FALSE}
ggscatterstats(data = dt, x = Total.Household.Income, y = Total.Food.Expenditure)
```


